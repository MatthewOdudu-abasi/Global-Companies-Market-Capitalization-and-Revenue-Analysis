
from bs4 import BeautifulSoup 

import requests

page = requurl = 'https://stockanalysis.com/list/biggest-companies/'
requests.get(url)
soup = BeautifulSoup(page.text, 'html')
print(soup)

soup.find('table')
soup.find_all('table')[0]

import pandas as pd

url = "https://stockanalysis.com/list/biggest-companies/"


df = tables[0]          # the big table
print(df.shape)         # should be 5497 rows (or close)
df.head()



import os
import requests
import pandas as pd
from bs4 import BeautifulSoup

URL = "https://stockanalysis.com/list/biggest-companies/?__v=1768911882277"

HEADERS = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
}

SAVE_FOLDER = r"C:\Users\Matthew odudu-abasi\Desktop\pandas"
OUTPUT_FILE = os.path.join(SAVE_FOLDER, "stockanalysis_biggest_companies_1000.csv")

def main():
    os.makedirs(SAVE_FOLDER, exist_ok=True)

    r = requests.get(URL, headers=HEADERS, timeout=60)
    r.raise_for_status()
    print("Status code:", r.status_code)

    soup = BeautifulSoup(r.text, "lxml")

    table = soup.find("table")
    if table is None:
        raise RuntimeError("No <table> found on the page. The site may have changed or blocked the request.")

    # Headers
    thead = table.find("thead")
    if thead:
        headers = [th.get_text(" ", strip=True) for th in thead.find_all("th")]
    else:
        first_row = table.find("tr")
        headers = [th.get_text(" ", strip=True) for th in first_row.find_all(["th", "td"])]

    headers = [h.replace("\n", " ").strip() for h in headers]
    headers = [f"Column_{i+1}" if (h is None or h.strip() == "") else h for i, h in enumerate(headers)]

    # Rows
    tbody = table.find("tbody")
    tr_list = tbody.find_all("tr") if tbody else table.find_all("tr")

    data_rows = []
    for tr in tr_list:
        tds = tr.find_all("td")
        if not tds:
            continue
        row = [td.get_text(" ", strip=True) for td in tds]
        data_rows.append(row)

    df = pd.DataFrame(data_rows, columns=headers[:len(data_rows[0])])

    # Keep first 1000 rows only
    df_1000 = df.head(1000).copy()

    print("Rows saved:", df_1000.shape[0])
    print("Columns saved:", df_1000.shape[1])

    df_1000.to_csv(OUTPUT_FILE, index=False, encoding="utf-8-sig")
    print("File saved to:", OUTPUT_FILE)



import os
import pandas as pd
import requests

url = "https://stockanalysis.com/list/biggest-companies/"

headers = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
    "Accept-Language": "en-US,en;q=0.9",
    "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8",
}

# 1) Download the page
resp = requests.get(url, headers=headers, timeout=60)
resp.raise_for_status()
print("Status code:", resp.status_code)

# 2) Extract the HTML table into a DataFrame
tables = pd.read_html(resp.text)
df = tables[0].copy()

# 3) Clean column names (optional but recommended)
df.columns = [str(c).replace("\n", " ").strip() for c in df.columns]

# 4) Confirm row count (must be >= 1000)
print("Shape (rows, cols):", df.shape)
if df.shape[0] < 1000:
    raise ValueError(f"Row count is {df.shape[0]} (< 1000). Page structure may have changed.")

# 5) Save (most reliable: saves into your current Jupyter folder)
out_csv = "biggest_companies_stockanalysis.csv"
df.to_csv(out_csv, index=False)
print("Saved here:", os.path.abspath(out_csv))

# 6) Optional: Save to Desktop\\pandas (creates folder if missing)
desktop_folder = r"C:\Users\Matthew odudu-abasi\Desktop\pandas"
os.makedirs(desktop_folder, exist_ok=True)

out_csv_desktop = os.path.join(desktop_folder, "biggest_companies_stockanalysis.csv")
df.to_csv(out_csv_desktop, index=False)
print("Saved to Desktop folder:", out_csv_desktop)

# 7) Display
df.head()

import requests
import pandas as pd
import os

url = "https://stockanalysis.com/list/biggest-companies/"

headers = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
}

page = requests.get(url, headers=headers)
print("Status code:", page.status_code)

soup = BeautifulSoup(page.text, "lxml")

import pandas as pd
df = pd.read_html(page.text)[0]
df.head()

table = soup.find("table")

import requests
import pandas as pd

url = 'https://stockanalysis.com/list/biggest-companies/?__v=1768911882277'
headers = {"User-Agent": "Mozilla/5.0"}

r = requests.get(url, headers=headers)
df = pd.read_html(r.text)[0]
print("First No. =", df.iloc[0,0])

import requests
import pandas as pd

base_url = "https://stockanalysis.com/list/biggest-companies/"
headers = {"User-Agent": "Mozilla/5.0"}

def get_df(u):
    r = requests.get(u, headers=headers, timeout=60)
    r.raise_for_status()
    df = pd.read_html(r.text)[0]
    df.columns = [str(c).replace("\n"," ").strip() for c in df.columns]
    return df

# Page 1
df1 = get_df(base_url)

# Try common page-2 patterns
candidates = [
    base_url + "?p=2",
    base_url + "?page=2",
]

df2 = None
for u in candidates:
    temp = get_df(u)
    first_no = int(str(temp.iloc[0,0]).replace(",","").strip())
    if first_no == 501:
        df2 = temp
        print("Using page 2:", u)
        break

if df2 is None:
    raise RuntimeError("Could not detect page 2. Open the site, click Next, and copy the URL from the address bar.")

# Merge to 1000 rows
df_1000 = pd.concat([df1, df2], ignore_index=True).head(1000)
print("Rows:", len(df_1000))

df_1000.to_csv("stockanalysis_1000_rows.csv", index=False)
df_1000

















